{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /home/josecaloca/multiclass-text-classification/src to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the absolute path to the src folder\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '../src'))\n",
    "\n",
    "# Add src to sys.path\n",
    "sys.path.append(src_path)\n",
    "\n",
    "print(f'Added {src_path} to sys.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./../settings.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from config import config\n",
    "\n",
    "tokenized_dataset_dict = load_dataset(config.hf_dataset_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 337, 'validation': 42, 'test': 42}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Define fraction reduction mapping for each split\n",
    "sample_fractions = {\n",
    "    'train': config.frac_sample_reduction_training,\n",
    "    'validation': config.frac_sample_reduction_training,\n",
    "    'test': config.frac_sample_reduction_training,\n",
    "}\n",
    "\n",
    "# Apply subsampling to each dataset split\n",
    "tokenized_dataset_dict = DatasetDict(\n",
    "    {\n",
    "        split: dataset.shuffle(seed=config.random_state).select(\n",
    "            range(int(dataset.num_rows * sample_fractions[split]))\n",
    "        )\n",
    "        for split, dataset in tokenized_dataset_dict.items()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print new dataset sizes\n",
    "print({split: ds.num_rows for split, ds in tokenized_dataset_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 16:18:26.515366: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-14 16:18:26.515450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-14 16:18:26.549611: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 16:18:26.628651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-14 16:18:28.100138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718a08c5f4e04e7eb4ce0037ac4ff811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638057f0a56d4afc9fecf257f3f91e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91201b944ce84cd186fd994f2b5d982b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (337, 768) (337,)\n",
      "Validation set shape: (42, 768) (42,)\n",
      "Test set shape: (42, 768) (42,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "# Load DistilBERT model\n",
    "model = DistilBertModel.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def extract_embeddings(batch):\n",
    "    # Convert input_ids and attention_mask to tensors (handling variable-length sequences)\n",
    "    input_ids = [torch.tensor(seq) for seq in batch['input_ids']]\n",
    "    attention_mask = [torch.tensor(seq) for seq in batch['attention_mask']]\n",
    "\n",
    "    # Pad sequences to the longest length within the batch\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=0\n",
    "    )\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        attention_mask, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Mean Pooling over tokens\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    return {'embeddings': embeddings}\n",
    "\n",
    "\n",
    "# Apply function to extract embeddings\n",
    "tokenized_dataset_dict = tokenized_dataset_dict.map(\n",
    "    extract_embeddings, batched=True, batch_size=32\n",
    ")\n",
    "\n",
    "# Convert to NumPy arrays for XGBoost training\n",
    "X_train = np.array(tokenized_dataset_dict['train']['embeddings'])\n",
    "y_train = np.array(tokenized_dataset_dict['train']['label'])\n",
    "\n",
    "X_val = np.array(tokenized_dataset_dict['validation']['embeddings'])\n",
    "y_val = np.array(tokenized_dataset_dict['validation']['label'])\n",
    "\n",
    "X_test = np.array(tokenized_dataset_dict['test']['embeddings'])\n",
    "y_test = np.array(tokenized_dataset_dict['test']['label'])\n",
    "\n",
    "print('Training set shape:', X_train.shape, y_train.shape)\n",
    "print('Validation set shape:', X_val.shape, y_val.shape)\n",
    "print('Test set shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame shape: (337, 769)\n",
      "Validation DataFrame shape: (42, 769)\n",
      "Test DataFrame shape: (42, 769)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert training data\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_train['label'] = y_train\n",
    "\n",
    "# Convert validation data\n",
    "df_val = pd.DataFrame(X_val)\n",
    "df_val['label'] = y_val\n",
    "\n",
    "# Convert test data\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['label'] = y_test\n",
    "\n",
    "# Print shapes\n",
    "print('Train DataFrame shape:', df_train.shape)\n",
    "print('Validation DataFrame shape:', df_val.shape)\n",
    "print('Test DataFrame shape:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josecaloca/multiclass-text-classification/src/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [16:30:37] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.28713\n",
      "[1]\tvalidation_0-mlogloss:1.20621\n",
      "[2]\tvalidation_0-mlogloss:1.14320\n",
      "[3]\tvalidation_0-mlogloss:1.09335\n",
      "[4]\tvalidation_0-mlogloss:1.03713\n",
      "[5]\tvalidation_0-mlogloss:0.99101\n",
      "[6]\tvalidation_0-mlogloss:0.94520\n",
      "[7]\tvalidation_0-mlogloss:0.91010\n",
      "[8]\tvalidation_0-mlogloss:0.87892\n",
      "[9]\tvalidation_0-mlogloss:0.84943\n",
      "[10]\tvalidation_0-mlogloss:0.81986\n",
      "[11]\tvalidation_0-mlogloss:0.79927\n",
      "[12]\tvalidation_0-mlogloss:0.77653\n",
      "[13]\tvalidation_0-mlogloss:0.75074\n",
      "[14]\tvalidation_0-mlogloss:0.73523\n",
      "[15]\tvalidation_0-mlogloss:0.71833\n",
      "[16]\tvalidation_0-mlogloss:0.70347\n",
      "[17]\tvalidation_0-mlogloss:0.69272\n",
      "[18]\tvalidation_0-mlogloss:0.68468\n",
      "[19]\tvalidation_0-mlogloss:0.66858\n",
      "[20]\tvalidation_0-mlogloss:0.66532\n",
      "[21]\tvalidation_0-mlogloss:0.65765\n",
      "[22]\tvalidation_0-mlogloss:0.64344\n",
      "[23]\tvalidation_0-mlogloss:0.63722\n",
      "[24]\tvalidation_0-mlogloss:0.62654\n",
      "[25]\tvalidation_0-mlogloss:0.62274\n",
      "[26]\tvalidation_0-mlogloss:0.61239\n",
      "[27]\tvalidation_0-mlogloss:0.60781\n",
      "[28]\tvalidation_0-mlogloss:0.59816\n",
      "[29]\tvalidation_0-mlogloss:0.59327\n",
      "[30]\tvalidation_0-mlogloss:0.58697\n",
      "[31]\tvalidation_0-mlogloss:0.57850\n",
      "[32]\tvalidation_0-mlogloss:0.57271\n",
      "[33]\tvalidation_0-mlogloss:0.57108\n",
      "[34]\tvalidation_0-mlogloss:0.56561\n",
      "[35]\tvalidation_0-mlogloss:0.56633\n",
      "[36]\tvalidation_0-mlogloss:0.56029\n",
      "[37]\tvalidation_0-mlogloss:0.55430\n",
      "[38]\tvalidation_0-mlogloss:0.55492\n",
      "[39]\tvalidation_0-mlogloss:0.55103\n",
      "[40]\tvalidation_0-mlogloss:0.55137\n",
      "[41]\tvalidation_0-mlogloss:0.54608\n",
      "[42]\tvalidation_0-mlogloss:0.54208\n",
      "[43]\tvalidation_0-mlogloss:0.54378\n",
      "[44]\tvalidation_0-mlogloss:0.54170\n",
      "[45]\tvalidation_0-mlogloss:0.53791\n",
      "[46]\tvalidation_0-mlogloss:0.53573\n",
      "[47]\tvalidation_0-mlogloss:0.53620\n",
      "[48]\tvalidation_0-mlogloss:0.53595\n",
      "[49]\tvalidation_0-mlogloss:0.53366\n",
      "[50]\tvalidation_0-mlogloss:0.53144\n",
      "[51]\tvalidation_0-mlogloss:0.52840\n",
      "[52]\tvalidation_0-mlogloss:0.52484\n",
      "[53]\tvalidation_0-mlogloss:0.52327\n",
      "[54]\tvalidation_0-mlogloss:0.52354\n",
      "[55]\tvalidation_0-mlogloss:0.52244\n",
      "[56]\tvalidation_0-mlogloss:0.52180\n",
      "[57]\tvalidation_0-mlogloss:0.51910\n",
      "[58]\tvalidation_0-mlogloss:0.51925\n",
      "[59]\tvalidation_0-mlogloss:0.51972\n",
      "[60]\tvalidation_0-mlogloss:0.51798\n",
      "[61]\tvalidation_0-mlogloss:0.51631\n",
      "[62]\tvalidation_0-mlogloss:0.51575\n",
      "[63]\tvalidation_0-mlogloss:0.51441\n",
      "[64]\tvalidation_0-mlogloss:0.51267\n",
      "[65]\tvalidation_0-mlogloss:0.51306\n",
      "[66]\tvalidation_0-mlogloss:0.51228\n",
      "[67]\tvalidation_0-mlogloss:0.51160\n",
      "[68]\tvalidation_0-mlogloss:0.51125\n",
      "[69]\tvalidation_0-mlogloss:0.51114\n",
      "[70]\tvalidation_0-mlogloss:0.51200\n",
      "[71]\tvalidation_0-mlogloss:0.51042\n",
      "[72]\tvalidation_0-mlogloss:0.51172\n",
      "[73]\tvalidation_0-mlogloss:0.51354\n",
      "[74]\tvalidation_0-mlogloss:0.51294\n",
      "[75]\tvalidation_0-mlogloss:0.51260\n",
      "[76]\tvalidation_0-mlogloss:0.51223\n",
      "[77]\tvalidation_0-mlogloss:0.51245\n",
      "[78]\tvalidation_0-mlogloss:0.51054\n",
      "[79]\tvalidation_0-mlogloss:0.51007\n",
      "[80]\tvalidation_0-mlogloss:0.51098\n",
      "[81]\tvalidation_0-mlogloss:0.50917\n",
      "[82]\tvalidation_0-mlogloss:0.50922\n",
      "[83]\tvalidation_0-mlogloss:0.50743\n",
      "[84]\tvalidation_0-mlogloss:0.50602\n",
      "[85]\tvalidation_0-mlogloss:0.50629\n",
      "[86]\tvalidation_0-mlogloss:0.50767\n",
      "[87]\tvalidation_0-mlogloss:0.50749\n",
      "[88]\tvalidation_0-mlogloss:0.50797\n",
      "[89]\tvalidation_0-mlogloss:0.50619\n",
      "[90]\tvalidation_0-mlogloss:0.50501\n",
      "[91]\tvalidation_0-mlogloss:0.50371\n",
      "[92]\tvalidation_0-mlogloss:0.50483\n",
      "[93]\tvalidation_0-mlogloss:0.50468\n",
      "[94]\tvalidation_0-mlogloss:0.50444\n",
      "[95]\tvalidation_0-mlogloss:0.50543\n",
      "[96]\tvalidation_0-mlogloss:0.50369\n",
      "[97]\tvalidation_0-mlogloss:0.50364\n",
      "[98]\tvalidation_0-mlogloss:0.50503\n",
      "[99]\tvalidation_0-mlogloss:0.50432\n",
      "Test Accuracy: 0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convert embeddings to DataFrames\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_train['label'] = y_train\n",
    "\n",
    "df_val = pd.DataFrame(X_val)\n",
    "df_val['label'] = y_val\n",
    "\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['label'] = y_test\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = df_train.drop(columns=['label']), df_train['label']\n",
    "X_val, y_val = df_val.drop(columns=['label']), df_val['label']\n",
    "X_test, y_test = df_test.drop(columns=['label']), df_test['label']\n",
    "\n",
    "# Define XGBoost model for multiclass classification\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # Multiclass classification\n",
    "    num_class=4,  # Number of classes\n",
    "    eval_metric='mlogloss',  # Multiclass log loss\n",
    "    eta=0.1,  # Learning rate\n",
    "    max_depth=6,  # Tree depth\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Load pretrained DistilBERT\n",
    "model_name = config.pre_trained_bert_model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "bert_model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.pre_trained_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in range(10):\n",
    "    print(len(tokenized_dataset_dict['train']['input_ids'][id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def extract_embeddings(dataset_dict: DatasetDict, model, batch_size=32):\n",
    "    \"\"\"Extract DistilBERT embeddings for input_ids in a DatasetDict, handling variable-length sequences.\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for split in dataset_dict.keys():\n",
    "            all_embeddings = []\n",
    "            all_labels = []\n",
    "            dataset = dataset_dict[split]\n",
    "\n",
    "            for i in range(0, len(dataset), batch_size):\n",
    "                batch = dataset[i : i + batch_size]\n",
    "\n",
    "                # Convert to torch tensors\n",
    "                input_ids = [torch.tensor(ids) for ids in batch['input_ids']]\n",
    "                attention_mask = [\n",
    "                    torch.tensor(mask) for mask in batch['attention_mask']\n",
    "                ]\n",
    "\n",
    "                # Pad sequences to max length in batch\n",
    "                input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "                attention_mask = pad_sequence(\n",
    "                    attention_mask, batch_first=True, padding_value=0\n",
    "                )\n",
    "\n",
    "                # Pass through BERT model\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "                # Extract [CLS] token (first token) as the embedding\n",
    "                cls_embeddings = last_hidden_states[:, 0, :].cpu().numpy()\n",
    "\n",
    "                all_embeddings.append(cls_embeddings)\n",
    "                all_labels.extend(batch['label'])\n",
    "\n",
    "            embeddings[split] = {\n",
    "                'X': np.vstack(all_embeddings),\n",
    "                'y': np.array(all_labels),\n",
    "            }\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings_dict = extract_embeddings(tokenized_dataset_dict, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [\n",
    "    torch.tensor(tokenized_dataset_dict['train']['input_ids'][0]),\n",
    "    torch.tensor(tokenized_dataset_dict['train']['input_ids'][1]),\n",
    "]\n",
    "attention_mask = [\n",
    "    torch.tensor(tokenized_dataset_dict['train']['attention_mask'][0]),\n",
    "    torch.tensor(tokenized_dataset_dict['train']['attention_mask'][1]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bert_model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states[:, 0, :].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Load DistilBERT tokenizer and model\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text = ['This is an example sentence.', 'Another sentence for embedding extraction.']\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Forward pass to get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings from the last hidden state\n",
    "embeddings = (\n",
    "    outputs.last_hidden_state\n",
    ")  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# Optionally, use the mean of all token embeddings as a sentence embedding\n",
    "sentence_embeddings = embeddings.mean(dim=1)  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "print('Embedding shape:', sentence_embeddings.shape)  # Should be (num_sentences, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'josecaloca/multiclass-text-classification'\n",
    ")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    'sentiment-analysis', model='josecaloca/multiclass-text-classification'\n",
    ")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
