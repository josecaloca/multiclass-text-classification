# ğŸ“° News Classification System

This project is a **news classification system** that processes and classifies news headlines into predefined categories using **machine learning models**. It is composed of three **Dockerized services** that ensure replicability and modularity.

## ğŸ“Œ Project Overview

The system consists of the following components:

1. **Data Preparation** - Downloads and preprocesses the dataset, cleaning and structuring it for training.
2. **Model Training** - Trains machine learning models, including fine-tuning a **DistilBERT** model and training an **XGBoost** classifier.
3. **API Service** - Deploys the trained model as a REST API for real-time news classification.

Additionally, a **client script** fetches live news headlines and sends them to the API for classification.

## ğŸ—ï¸ Setup & Execution

Each service is containerized with **Docker** and contains a `Makefile` to simplify execution. Docker images are available in the following DockerHub repository: https://hub.docker.com/r/josecaloca/multiclass-text-classification

Using `make` from the `Makefile`, we can simplify the execution of the project. It is recommended to see the available `make` commands by running:

```bash
make help
```
In overall, for building the Docker images and running the containers, 1 command is needed:

```bash
make build run
```

Alternatively, we can build the Docker images and run the containers of each service as follows:

### 1ï¸âƒ£ Run Data Preparation
```bash
make build-data-prep run-data-prep
```

### 2ï¸âƒ£ Run Model Training
```bash
make build-model-train run-model-train
```

### 3ï¸âƒ£ Run API Service
```bash
make build-api run-api
```

**Note**: Once the API is running, it will be available at: ```http://127.0.0.1:8000```

## ğŸš€ Using the System

### ğŸ” Classify News Headlines
A client script (`./client.py`) fetches real-time news headlines from an external news API and classifies them using the deployed model.

Run the client script:
```cd
python client.py
```

### ğŸ› ï¸ API Endpoints

The API exposes the following endpoint:

- POST /predict
    - Description: Classifies a news headline into a category.
    - Payload: `{"title": "Some news headline"}`
    - Response:
    ```json
    {"category": "business"}
    ```

## ğŸ“‚ Project Structure (High-Level)

```bash
.
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ client.py               # Fetches news headlines and classifies them
â”œâ”€â”€ services                # The three main services
â”‚   â”œâ”€â”€ api                 # API service for model inference
â”‚   â”œâ”€â”€ data_preparation    # Data preprocessing pipeline
â”‚   â””â”€â”€ model_training      # Training pipeline for machine learning models

```

## ğŸ”§ Key Technologies

This project utilizes a modern **MLOps pipeline** with well-structured **Dockerized services** to ensure reproducibility and efficient deployment. Below are the key technologies used:

### ğŸ“Œ **Machine Learning & Model Training**
- **Hugging Face Transformers** â€“ Used for **fine-tuning a DistilBERT model** to classify news headlines.
- **XGBoost** â€“ A gradient boosting algorithm used for training an alternative classifier based on embeddings extracted from DistilBERT.
- **Scikit-Learn** â€“ Used for additional preprocessing and evaluation metrics.
- **CometML** â€“ Integrated to **track experiments**, log training metrics during model development and used also as a secondary model registry.

### ğŸ“¦ **Model Registry & Feature Store (Hugging Face Hub + CometML)**
- **Hugging Face Hub** acts as:
  - A **Model Registry** for storing and versioning **fine-tuned DistilBERT models**
  - A **Feature Store** where the **pre-processed dataset** (tokenized) is stored and retrieved for training and inference.
- The trained XGBoost model is stored in **CometML**.

### ğŸš€ **Model Deployment & API**
- **Litserve** â€“ A high-performance framework built on **FastAPI**, optimized specifically for **ML model deployment**.
- **Requests** â€“ Used in `client.py` to fetch live news headlines and send them to the API for classification.

### ğŸ³ **Containerization & Automation**
- **Docker** â€“ Ensures each service runs in an isolated and reproducible environment.
- **Makefiles** â€“ Automates the **build & run** process for each service (`data_preparation`, `model_training`, and `api`).
- **UV** â€“ Used as the **dependency manager**, ensuring reproducibility and fast installations (instead of Poetry).

### ğŸ“Š **Experiment Tracking & Logging**
- **CometML** â€“ Used to log metrics such as **F1-score, precision, recall, and confusion matrices** during training.
- **Loguru** â€“ A modern logging library used for structured logging and error tracking across the project.

### ğŸ“œ **Code Quality & Linting**
- **Ruff** â€“ A fast and efficient Python linter that enforces best practices and keeps the code clean.
- **Pre-commit Hooks** â€“ Ensures code quality by automatically running linting, formatting, and security checks before commits.
